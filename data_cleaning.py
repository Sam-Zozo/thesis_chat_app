import time
from tokenize import Special
from data.filipino_stopwords import filipino_stopwords
from data.tagalog_words import tagalog_words
from data.raw_profanity import raw_profanity
# from data.tagalog_words2 import tagalog_words as t2
from nltk import regexp_tokenize
import nltk
from stemmer import stemmer
from jaro_winkler import jaro_Winkler

VOWELS = "aeiouAEIOU"
CONSONANTS = "bdghklmnngprstwyBDGHKLMNNGPRSTWY"
NUMBERS = "1234567890"

alt_chars = {
    'a':["Ã ","4","@","^",'Ã¡',"Î»","âˆ‚","ae","Ã¤","*"],
    'b':['8','â’','àµ«','ÃŸ','á¸ƒ','v'],
    'd':['á¸‹','Ä','á¸Š','á¸','á¸“','cl','Ã','âˆ‚','Ã°','É—'],
    'e':['3','&','Ã©','â‚¬','ii','É™','Â£','*','É‡'],
    'f':['Æ’','}','f','Æ‘','á¸Ÿ','á¸ž','â“•'],   
    'g':['6','9','q','Ä','Ç§','á¸¡','Ä£','Ç¥','É '],
    'h':['å„','#','á¸¥','á¸«','â±¨','á¸£','Ä¥','ÈŸ','Ä§'], 
    'i':['!','1','|','*','â€¡','ð“²','/'],
    'j':['dy','Äµ','Ä´','Ç°','É‰','Éˆ'],
    'k':['q','É®','c'],
    'l':['Å‚','1','|','Â£','Â¬'],
    'm':['á¸¿','á¹','á¹ƒ','mÌ€','áµ¯'],
    'n':['Ã±','Å„','Å†','É²','Å‹','á¹…','~','â‚ª'],
    'o':['Ã´','Ã¶','Ã²','Ã³','Å“','Ã¸','Å','Ãµ','0','Â¤','Î©','Ï‰','*'],
    'p':['f','q','â‚±','â„—','Ã¾','Â¶'],
    'r':['2','Â®','Ð¯','Ê'],
    's':['5','ÃŸ','Å›','Åž','$','z','es','Êƒ','Â§','Å¡'],
    't':['7','Å‚','+','ä¸…''1','â€ '],
    'u':['v','Ã»','Ã¹','Å«','Ãº','Ã¼'],
    'v':['ÊŒ','\/','âˆš','l/','|/'],
    'w':['Ð¨','É°'],
    'y':['á»·','`/','Î¨','Ï†','Î»','Ð§','Â¥','Ã¿'],

}

def whitespace_tokenizer(string):
    white_space_tokenizer = string.split(' ')
    return dict.fromkeys(white_space_tokenizer)

def to_lowercase(string):
    return string.lower()

def leet_checker(tokens):
    temp = tokens
    for key in temp:
        if is_leet(key): # if not normal word
            temp[key] = {'isLeet':  True, 'originalWord': str(word_leet_to_tagalog(key))}
            
        else:
            temp[key] = {'isLeet':  False}
    
    # for key ,value in temp.items():
    #     print(key, value)
    return temp

def is_leet(word):
    c = 'wertyuiopasdghklbnm'
    if word.isnumeric():
        return False
    for char in word:
        if not char in c:
            return True
    return False
    # if not word.isalpha(): 
    #     return True
    # return False

# character level translation of leet to tagalog
def word_leet_to_tagalog(word):
    bWord = word
    for char in word:
        char_leet_equivalent_dict = filter_the_dict(alt_chars, lambda elem: char in elem[1])
        if char_leet_equivalent_dict: # if not empty
            key = [k for k,v in char_leet_equivalent_dict.items() if char in v]
            bWord = bWord.replace(char,key[0])
    return bWord

def filter_the_dict(dictObj, callback):
    newDict = dict()
    # Iterate over all the items in dictionary
    for (key, value) in dictObj.items():
        # Check if item satisfies the given condition then add to new dict
        if callback((key, value)):
            newDict[key] = value
    return newDict

# list -> check if leet -> if leet translate -> perform stemming -> tag stopwords -> tag tagalog words -> jaro
def is_tagalog_characters(word):
    tag_char = 'wertyuiopasdghklbnm'
    isAllTagChar = False
    for i in word:
        if i in tag_char:
            isAllTagChar = True
    return isAllTagChar

def tagalog_stemmer(tokens):
    l = list()
    temp = tokens
    for key, value in temp.items():
        if not value['isLeet']:
            l.append(key)
        else:
            l.append(value['originalWord'])
    # print(l)
    ls = stemmer('3', l, '1')
    # print("ls:",ls[1])
    for key, value in tokens.items():
        for dictionary in ls[0]:
            
            if dictionary['word'] == key: #nakapa typo?
                temp[key]['rootWord'] = dictionary['root']
            elif value.get('originalWord') and dictionary['word'] == value['originalWord']:
                temp[key]['rootWord'] = dictionary['root']
            # elif value.get('originalWord') and dictionary['word'] != value['originalWord']:
            #     temp[key]['rootWord'] = value['originalWord']
            # else:
            #     temp[key]['rootWord'] = ''
    return temp

def stopwords_checker(tokens):

    temp=tokens
    # print("test",temp)
    for key, value in tokens.items():
        toSearch = key
        # print('r = ',value['rootWord'])
        if toSearch in filipino_stopwords or len(toSearch) <= 3:
            temp[key]['isStopword'] = True
            continue
        if value['rootWord'] and value['rootWord'] != key:
            toSearch = value['rootWord']
        if toSearch in filipino_stopwords or len(toSearch) <= 3:
            temp[key]['isStopword'] = True
        else:
            temp[key]['isStopword'] = False
    return temp
def isAllVowels(word):
    count = 0
    for c in word:
        if c in VOWELS:
            count+=1
    if count == len(word):
        return True
    else:
        return False

def filipino_word_checker(tokens):
    temp=tokens

    for key, value in tokens.items():
        isWordToSearch = key
        if isAllVowels(key):
            temp[key]['isStopword'] = True
            continue
        if value['isStopword']:
            temp[key]['isDictionaryWord'] = True
            continue
        if binary_search2(tagalog_words,isWordToSearch) > 0: 
            temp[key]['isDictionaryWord'] = True
            continue
        # if value.get('orignalWord') and (binary_search2(tagalog_words,value.get('orignalWord')) )> 0: 
        #     temp[key]['isDictionaryWord'] = True
        #     continue
        isWordToSearch = value['rootWord']
        if binary_search2(tagalog_words,isWordToSearch) > 0: 
            temp[key]['isDictionaryWord'] = True
        else:
            temp[key]['isDictionaryWord'] = False
    return temp

def binary_search(arr, lower_bound, upper_bound, word):
    if upper_bound >= lower_bound:
        mid = (upper_bound + lower_bound) // 2
        if arr[mid] == word:
            return mid
        elif arr[mid] > word:
            return binary_search(arr, lower_bound, mid - 1, word)
        else:
            return binary_search(arr, mid + 1, upper_bound, word)
    else:
        return -1   

def binary_search2(arr, x):
    low = 0
    high = len(arr) - 1
    mid = 0
    while low <= high:
        mid = (high + low) // 2
        if arr[mid] < x:
            low = mid + 1
        elif arr[mid] > x:
            high = mid - 1
        else:
            return mid
    return -1

def init_default_values(tokens):
    for key in tokens.keys():
        # tokens[key]['isLeet'] = False
        # tokens[key]['originalWord'] = ''
        # tokens[key]['isStopword'] = False
        # tokens[key]['isDictionaryWord'] = False
        # tokens[key]['rootWord'] = ''
        tokens[key]['isProfane'] = False

    return tokens

def clean_text(string):
    threshold = 0.8
    string = to_lowercase(string)
    tokens = init_default_values(filipino_word_checker(stopwords_checker(tagalog_stemmer(leet_checker(whitespace_tokenizer(string))))))
    # tokens = init_default_values(filipino_word_checker(stopwords_checker(leet_checker(whitespace_tokenizer(string)))))
    for key, value in tokens.items():
        if value['isStopword'] == False and value['isDictionaryWord'] == False: #if not stopword AND not in dictionary
            x = [x for x in raw_profanity if jaro_Winkler(value['rootWord'],x) >= threshold]
            if x:
                tokens[key]['isProfane']  = True
                print(x)
            else:
                tokens[key]['isProfane']  = False
            # if value['isLeet'] == True:
            #     x = [x for x in raw_profanity if jaro_Winkler(value['originalWord'],x) >= threshold]
            #     if x:
            #         tokens[key]['isProfane']  = True
               # print(x)
            #     else:
            #         tokens[key]['isProfane']  = False
            # else:
            #     x = [x for x in raw_profanity if jaro_Winkler(key,x) >= threshold]
            #     if x:
            #         tokens[key]['isProfane']  = True
            #         print(x)
            #     else:
            #         tokens[key]['isProfane']  = False
        else:
            continue
    return tokens

if __name__ == "__main__":
    # sentence = 'ang p3tsa g@g0 ay pebrero ng Tite-sais,  d@law@ng l!bo\'t kantotan dos Unshaded votes and votes for Mayor Duterte goes to Mar Roxas according to some reports of ballot tests.  #AyawSaDILAW,1Na-Binay ??????'
    sentence = 'bobohan pakyu kaululan gaggggaaaa' # bbm 88m ibon putang-!na mo na lumilipad ay t4rant@do odatnarat ogag G@go ka hinayup4k ka'
    # sentence = "putangina edi gawin mong manok gago ampota"
    # stopwords = ' '.join(filipino_stopwords)
    
    #sentence  = 'kagastos nak@kasik@t ng tang!na ang napakasakit nakakaantok'
    start = time.time()    
    # sentence = 'puta ina mo olol'
    tokens = clean_text(sentence)
    
    end = time.time()
    # for key, value in tokens.items():
    #     print(key, value)
    print('time: ', end - start)

    print(sentence)
    for key, value in tokens.items():
        # if value['isProfane'] == True:
            print(key, value)

# aeiou aaaaa oo aoie 
# print('817'.isnumeric())
# print(string.punctuation)
# print(word_leet_to_tagalog('pu7@n6!na'))
# title = 'es and votes for Mayor Duterte goes to Mar Roxas according to some reports of ballot tests.  #AyawSaDILAW,1Na-Binay ??????'
# sample = 'puta qaqo yayamanin jakol hhhh mahal pakiskis pakita g@go'
# tokens = whitespace_tokenizer(sample)
# tokens = leet_checker(tokens)
# tokens = stopwords_checker(tokens)
# tokens = filipino_word_checker(tokens)
# tokens = init_default_values(tokens)
# for key, value in tokens.items():
#         if value['isStopword'] == False and value['isDictionaryWord'] == False:
#             if value['isLeet'] == True:
#                 x = [x for x in raw_profanity if jaro_Winkler(value['originalWord'],x) > 0.78]
#                 if x:
#                     tokens[key]['isProfane']  = True
#                     print(x)
#                 else:
#                     tokens[key]['isProfane']  = False
#             else:
#                 x = [x for x in raw_profanity if jaro_Winkler(key,x) > 0.78]
#                 if x:
#                     tokens[key]['isProfane']  = True
#                     print(x)
#                 else:
#                     tokens[key]['isProfane']  = False
#         else:
#             continue
# for key ,value in tokens.items():
#     print(key, value)

